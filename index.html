<!DOCTYPE html>
<html>

<head>
    <title>EECS 351 Project</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <style type="text/css">
        body {
            padding-left: -40px;
            padding-right: -40px;
            padding-top: -100px;
        }
        
        .carousel img {
            position: absolute;
            top: 0;
            left: 0;
            min-width: 100%;
            height: 500px;
        }
        
        .carousel-caption {
            margin-botton: 100px;
            text-align: center;
        }
    </style>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } });
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js"></script>
</head>


<!-- ********************** Navbar ******************* -->
<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
        <button class="navbar-toggle" data-toggle="collapse" data-target=".navHeaderCollapse">
	      <span class="sr-only">Toggle navigation</span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	    </button>
        <a class="navbar-brand" href="#">Home</a>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="navbar-collapse collapse navHeaderCollapse">
            <ul class="nav navbar-nav">
                <li class=""><a href="#intro">Introduction</a></li>
                <li class=""><a href="#data">Data</a></li>
                <li class=""><a href="#algorithm">Alogorithm Overview</a></li>
                <li class=""><a href="#technical">Technical Work</a></li>
                <li class=""><a href="#results">Experimental Results</a></li>
                <li class=""><a href="#conclusions">Conclusions</a></li>
                <li class=""><a href="#demo">Demo</a></li>
                <li class=""><a href="#references">References</a></li>
                <li class=""><a href="progress1.html">Progress Report</a></li>
            </ul>
        </div>
    </div>
    <!-- /.navbar-collapse -->
</div>

<!-- *************** Body ***************** -->

<div class="jumbotron">
    <div class="container">
        <br><br><br><br><br><br><br><br><br><br>
        <h1>
            <center> Speaker Diarization</center>
        </h1>
        <br><br>
        <h2>
            <center>Robert Malinas and Samuel Rohrer</center>
        </h2>
        <br>
        <h3>
            <center>December 12, 2016</center>
        </h3>
        <br><br>
    </div>
</div>


<div class="container">

    <!-- Introduction  -->
    <div class="container">
        <a name="intro"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Introduction </h2>
        <p>We are Sam Rohrer and Bob Malinas. This website is dedicated to presenting our final EECS 351 project at the University of Michigan. Sam is a computer engineering major, and Bob an electrical engineering major.</p>
        <p> The goal of our project is to take a sample of human speech and determine who is speaking as a function of time. This is known as speaker diarization. Applications for speaker diarization include separating a recording of a business meeting into segments, where each segment is a specific person giving their portion of the meeting. This is to make the lives of people who have to listen to these meetings easier.  </p>
        <p> We aim to do this using Mel Frequency Cepstrum Coefficients (MFCCs), which are abstract feature vectors that are calculated using digital methods that mimic the inherent audio filtering of the human ear. Just as humans can determine who is speaking given a speech recording, the belief is that a digital process will be able to do the same. </p>
        <p> To accomplish our goal, we tested a variety of different speech features and classification methods. The method that produced the most accurate results was a classifier that used Mahalanobis distance and MFCCs. The specifics of these can be found in the section labeled Technical Work. An extensive overview of the output of each combination of features and classification methods can be viewed in the Experimental Results section.</p>
        <p> The results of our Mahalanobis classifier are very favorable, yielding highly accurate speaker classification within a resolution of one second. This means that, in any given second, the algorithm could be predicting an incorrect speaker; however, it will be correct in the next second. For long speech inputs, one second resolution is realistically good enough. Additionally, the algorithm can even work with segments of speech that have a significant amount of breaths in them. Breaths proved impossible to classify, so handling them involved some error correction. The specific methods of error correction are discussed in the Experimental Results section.</p>
    </div>

    <!-- Data  -->
    <div class="container">
        <a name="data"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Data </h2>
        <p> The primary source of data we used in our project was MFCC databases and speech recordings for a variety of speakers. For each speaker in the recordings, a database of MFCCs is required for the algorithm to work. To gather these databases, we recorded each person that speaks in the input speech signal separately, ran them through the MFCC algorithm, and placed their MFCCs in a matrix. An example of an MFCC vector is seen below to the right.</p>
        <p> In the preliminary brainstorming stages of the project, we examined spectrograms of different speakers. An example of such a spectrogram is seen below to the left. Despite not using spectrograms in the final algorithm, they served as a stepping stone to MFCCs. They are similar to MFCCs in that they also represent short-time frequency data. Understanding spectrograms and the windowing involved in generating them was a key step to understanding MFCCs. In the spectrogram below to the left, one speaker is talking. We used a Hamming window for each short-time Fourier transform to avoid spectral leakage. Additional spectrograms can be seen in our progress report.</p>
        <p> Finally, actual speech data can be seen in the center picture below. This speech data contains two speakers talking, and this was a speech sample that we used to test our algorithm during development. Again, the goal is to separate such speech signals into individual signals with only one speaker talking.</p>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/onespeakerhamming.png">
                        <img src="pics/onespeakerhamming.png" alt="">
                    </a>
                    <h4> One Speaker Hamming Window Spectrogram</h4>
                    <p> This is the spectrogram of a single speaker (~12 seconds long). It was computed with a Hamming window. We hypothesize the areas around 2 seconds and 11 seconds are fricative sounds based on their spread-spectrum frequency response. The areas around 8 seconds and 10 seconds are likely plosive sounds, as they are also spread spectrum but shorter in length. </p>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/twospeakersspeech.png">
                        <img src="pics/twospeakersspeech.png" alt="">
                    </a>
                    <h4>Speech signal with two speakers</h4>
                    <p> This is a speech signal to be inputted to the algorithm. The goal of the algorithm is to separate this speech signal into two separate signals, each with only one speaker talking. </p>
                </div>
            </div>

            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_plot.png">
                        <img src="pics/mfcc_plot.png" alt="">
                    </a>
                    <h4>MFC Coefficients Plot</h4>
                    <p> This is the plot of a single sample (30 ms) computed with a Hamming window. The signal was band-limited at 300 Hz and 5000 Hz, based on the frequency spectrum of typical speech. It was then binned into 25 mel frequency bins. The plot shown here is the result of the MFCC computation, with the x-axis as dimension and the y-axis as magnitude. </p>
                </div>
            </div>

        </div>
    </div>

    <!-- Algorithms  -->
    <div class="container">
        <a name="algorithm"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Algorithm Overview </h2>
        <div class="thumbnail">
            <a href="pics/pipeline.png">
                <img src="pics/pipeline.png" alt="">
            </a>
            <h4> Pipeline View of Algorithms Used</h4>
            <p> <a href="#envelopes"> Features: envelopes </a></p>
            <p> <a href="#mfcc"> Features: Mel Frequency Cepstrum Coefficients (MFCCs) </a></p>
            <p> <a href="#knn"> Classification: Kth Nearest Neighbors </a></p>
            <p> <a href="#mahalanobis"> Classification: Mahalanobis Distance </a></p>
            <p> <a href="#errorcorrection"> Error Correction </a></p>

        </div>
    </div>

    <!-- Technical Work  -->
    <div class="container">
        <a name="technical"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Technical Work </h2>
        <p> In this section we present the technical work conducted throughout the semester while trying to create a speaker diarization system. There are two sections, audio features and speaker classification. In audio features we discuss envelopes and MFCCs. In speaker classification, we discuss the $K^{th}$ Nearest Neighbors method and the Mahalanobis distance method. Note that not all algorithms discussed were used in the final speaker diarization system.
            <a name="envelopes"></a>
            <br>
            <h3> Audio Features </h3>
            <br>
            <h4> Envelopes </h4>
            <p> Based on reference [1], we know that peaks in the spectral envelope of the time domain signal are known as formants. Additionally, different speakers have different formants and different sounds also have different formants. We first computed the envelope of the absolute value of the signal. In order to identify the formants, we compute the energy under the envelope at six different thresholds. The thresholds include less and less of the signal to help identify the formants. The features used were (in MATLAB)
            : $$ \begin{equation} e_1 = trapz(envelope(envelope > thd)), thd = \frac{1}{2} \end{equation} $$ $$ \begin{equation} e_2 = trapz(envelope(envelope > thd)), thd = \frac{5}{9} \end{equation}$$ $$ \begin{equation} e_3 = trapz(envelope(envelope > thd)), thd = \frac{5}{8} \end{equation}$$ $$ \begin{equation} e_4 = trapz(envelope(envelope > thd)), thd = \frac{2}{3} \end{equation}$$ $$ \begin{equation} e_5 = trapz(envelope(envelope > thd)), thd = \frac{3}{4} \end{equation}$$ $$ \begin{equation} e_6 = trapz(envelope(envelope > thd)), thd = \frac{7}{8} \end{equation}$$ </p><br>

            <div class="row">
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/formant.png">
                            <img src="pics/formant.png" alt="">
                        </a>
                        <h4> Formants Example</h4>
                        <p> This diagram shows an example of speech formants, taken from
                            <a href="http://person2.sol.lu.se/SidneyWood/praate/images/formant03.gif">
                    link.</a></p>
                    </div>
                </div>
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/envelopeSample.png">
                            <img src="pics/envelopeSample.png" alt="">
                        </a>
                        <h4>Sample Envelope Coefficients</h4>
                        <p> These are sample envelope coefficients from Bob (blue) and Sam (red), you can see the difference between the two that should allow succesful speaker classification. </p>
                    </div>
                </div>
            </div>

            <a name="mfcc"></a>
            <br>
            <h4> Mel Frequency Cepstrum Coefficients (MFCCs) </h4>
            <p> Here we outline the steps required to generate MFCCs. MFCCs measure the short term log power spectrum of the signal. This is based on the Mel scale (see image below), which separates the frequency spectrum into non-linear bins which more closely mimic human hearing. The Mel scale achieves this with smaller bins (~100 Hz wide) at lower frequencies and larger bins (~1000 Hz wide) at higher frequencies.</p><br>

            <p> 1) Window the signal into 30 ms intervals with a 20 ms overlap, which means that there are 10 ms of new signal information for every window. Then a Hamming window was applied to prevent spectral leakage following (7) ($N=30$, $n=[1,30]$): $$ \begin{equation} W(n) = 0.54 + 0.46*cos(\frac{2\pi n}{N-1}) \end{equation} $$ </p> <br>
            <p> 2) Compute the Fast Fourier Transform (FFT) of the windowed signal: </p> <br>
            <p> 3) Compute the magnitude of the FFT coefficients: $$ \begin{equation} FFT_{mag} = |FFT|^2 \end{equation}$$ </p><br>
            <p> 4) Choose $f_{max} = 3000 Hz$, $f_{min} = 100 Hz$, and the number of Mel bins $Bins_{Mel} = 15$. Compute the Mel scale: $$\begin{equation} f_{max} Mels = 1125 * log(1 + \frac{f_{max} Hz}{700})\end{equation}$$ $$\begin{equation} f_{min} Mels = 1125 * log(1 + \frac{f_{min} Hz}{700})\end{equation}$$ </p><br>
            <p> 5) Using $f_{max}$ and $f_{min}$ in Mels, and the number of Mel bins create equispaced filters on the Mel scale. Next convert these Mel frequencies back to Hz to determine the location of the filters: $$\begin{equation} f_{filter} Hz = 700 * (e^{\frac{f_{filter} Mel}{1125}} - 1) \end{equation}$$ This creates a Mel filter bank similar to the image, "Sample Mel Filter Bank".</p><br>
            <p> 6) Compute the magnitude of $FFT_{mag}$ within each of the triangular Mel filter banks. </p><br>
            <p> 7) Compute the log of these magnitudes. Length of vector here should be equal to the number of Mel bins. </p><br>
            <p> 8) Generate the MFCCs by computing the Inverse FFT of the log of magnitudes. In this case the IFFT reduces to the Discrete Cosine Transform (DCT). The result should be similar to the image "Sample MFCCs".
            The effect of this is to make the coefficients purely real.</p><br>

            <div class="row">
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/melfilterbank.png">
                            <img src="pics/melfilterbank.png" alt="">
                        </a>
                        <h4> Sample Mel Filter Bank</h4>
                        <p> This diagram shows a sample mel filter bank, taken from
                            <a href="https://www.vocal.com/wp-content/uploads/2013/06/MelFilterBank.jpeg">
                    link.</a></p>
                    </div>
                </div>
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/mfccs.png">
                            <img src="pics/mfccs.png" alt="">
                        </a>
                        <h4>Sample MFCCs</h4>
                        <p> These are sample MFCCs from Bob (blue) and Sam (red), you can see the difference between the two that should allow succesful speaker classification. This was computed for 13 filter banks. </p>
                    </div>
                </div>
            </div>

            <br>
            <h3> Speaker Classification </h3>
            <a name="knn"></a>
            <br>
            <h4> Kth Nearest Neighbors </h4>
            <p>In this section, the $K^{th}$ Nearest Neighbors classification algorithm is discussed. This algorithm is very simple in complexity, however its lack of elegance is evident during implementation. It is a very brute force method of classification, and its results are not as good as those for the other classifier we developed. Additionally, it takes far longer to run. The exact runtimes are discussed in the Implementation subsection of the Mahalanobis Distance section.
            </p>

            <p>The idea behind $K^{th}$ Nearest Neighbors is to find, quite literally, the nearest $k$ vectors in an $N$-dimensional space using the Euclidean metric. In this case, these $N$-dimensional vectors are MFCC vectors. In this application, MFCC databases of each speaker are generated. That is, given a speech segment with only one of the speakers talking, a matrix whose columns are MFCC vectors, each corresponding to a short time interval (discussed in MFCC section), is generated. Then, given an input speech vector with these two speakers talking at different times, MFCC vectors are generated. Given a single input MFCC vector, the $K^{th}$ Nearest Neighbors algorithm compares this single MFCC vector with all of the vectors in both databases. Specifically, the Euclidean distance between the single input vector and every vector in both databases is calculated. Then, the database vectors with the $k$ smallest distances are recorded. The speaker whose vectors comprise the majority of these $k$ vectors is determined as who is speaking for the time interval from which this MFCC vector was generated. In our experiments with the $K^{th}$ Nearest Neighbors algorithm we tested several different values of $k$. We found that, no matter the value of $k$, the Mahalanobis classifier was much faster and more accurate, although sometimes marginally so. Due to the lesser accuracy of $K^{th}$ Nearest Neighbors and the much greater time cost, the final algorithm uses the Mahalanobis classifier.</p>
            <p>Below is a diagram demonstrating the $K^{th}$ Nearest Neighbors method for 3 different values of $k$. The red and green points represent 2-dimensional MFCC vectors for example red and green speaker databases. The blue vector is the input MFCC vector that is being compared. In this example, the blue point would be classified as the red speaker talking for all three values of $k$. The axes for this plot are simply contrived example MFCCs.</p>
            <br>
           
                        <a href="pics/kthnearestexample.png">
                            <img src="pics/kthnearestexample.png" alt="">
                        </a>
            <br>
            <a name="mahalanobis"></a>
            <br>
            <h4> Mahalanobis Distance </h4>
            <p>
                <h5><b>Introduction</b></h5>
                In this section, the Mahalanobis Distance and its utilization with MFCCs are discussed. Suppose the MFFCs for each speaker are distributed in an N-dimensional space, and there exists a unique distribution for each speaker. Then, it would be useful to determine how likely it is that an MFCC vector belongs to a specific speaker's distribution. During development, this idea was tested and confirmed. The next section presents the mathematics necessary to understand the Mahalanobis classifier.</p>

            <h5><b>Mathematics and Explanation</b></h5>
            <p>Consider a real N-dimensional column vector $ \mathbf{\vec{v}} $. Then, let $ \mathbf{S} $ be the covariance matrix of a real N-dimensional distribution and let $\boldsymbol{\vec{\mu}}$ be a vector containing the mean along each dimension of this distribution. The Mahalanobis Distance is defined as $$ \begin{equation} D_{M} = \sqrt{(\mathbf{\vec{v}} - \boldsymbol{\vec{\mu}})^{\,T}\mathbf{S}^{-1}(\mathbf{\vec{v}} - \boldsymbol{\vec{\mu}})}. \end{equation}$$ Let $$ \begin{equation} \mathbf{\vec{d}} = \mathbf{\vec{v}} - \boldsymbol{\vec{\mu}}. \end{equation}$$ Note that $\mathbf{\vec{d}}$ is simply the vector difference between the vector being compared to the distribution, $\mathbf{\vec{v}}$, and the mean vector, $\boldsymbol{\vec{\mu}}$. The importance of $\mathbf{\vec{d}}$ will become clear shortly. It is difficult to understand the Mahalanobis Distance from (12). To develop intuition, consider the relation $$ \begin{equation} \mathbf{S} = \mathbf{LL}^T, \end{equation}$$ where $\mathbf{L}$ is the Cholesky Decomposition of $\mathbf{S}$. This decomposition is guaranteed to exist because $\mathbf{S}$, being a covariance matrix, is positive semi-definite. After combining (12), (13), and (14), the result reduces to $$\begin{equation} D_{M} = \sqrt{
                <\mathbf{L^{-1}\vec{d}}, \mathbf{L^{-1}\vec{d}}>}, \end{equation}$$ which is simply the 2-norm of the vector $\mathbf{L^{-1}\vec{d}}$. Another important relation will clarify why the Mahalanobis distance works. Consider a completely uncorrelated N-dimensional random column vector $\mathbf{X}$ that has unit variance along each dimension. Let $$\begin{equation} \mathbf{I} = cov(\mathbf{X}). \end{equation}$$ The symbol $\mathbf{I}$ was chosen intentionally, as this covariance matrix is necessarily an N by N identity matrix. Now, suppose $\mathbf{X}$ is linearly transformed by a matrix $\mathbf{L}$. Again, the choice of the symbol $\mathbf{L}$ for the transformation matrix is intentional. Then, $$\begin{equation} cov(\mathbf{LX}) = \mathbf{L}cov(\mathbf{X})\mathbf{L}^T, \end{equation}$$ but (16) shows that $cov(\mathbf{X})$ is an identity matrix. Therefore, (17) becomes $$\begin{equation} cov(\mathbf{LX}) = \mathbf{LL}^T = \mathbf{S}. \end{equation}$$ This profound result shows that a random sample distribution of N-dimensional vectors with covariance matrix $ \mathbf{S}$ can be linearly transformed by the inverse Cholesky decomposition of $ \mathbf{S} $, namely $\mathbf{L^{-1}}$, to become an uncorrelated distribution that has unit variance along each dimension. This completely removes the bias caused by the distribution from each sample vector. Then, it is clear that the Mahalanobis Distance is simply 2-norm of the difference vector $\mathbf{\vec{d}}$ when viewed in the context of an orthonormal basis$^1$ that is chosen carefully to remove the bias of the distribution. For a less rigorous, intuitive description of the Mahalanobis distance, continue reading.</p>

            <h5><b>Intuitive Description</b></h5>
            <p> The Mahalanobis Distance is an N-dimensional analogue to the z-score, which tells how many standard deviations a point is away from the mean in a Gaussian distribution. The z-score of a sample point with respect to a distribution allows one to determine how likely it is that this sample point belongs to the distribution. If two z-scores are calculated for the sample point, each with respect to a unique distribution, then the point most likely belongs to the distribution with which it has the lesser z-score. The same principle can be applied using the Mahalanobis Distance for N-variate distributions. Given an N-dimensional sample vector and two distributions, the sample vector most likely belongs to the distribution with which it has a lesser Mahalanobis distance. Another way to view this is that the sample vector is "less of an outlier" in the distribution where its Mahalanobis distance is the smallest. The next section describes the Mahalanobis classifier.
            </p>

            <h5><b>Method of Classification</b></h5>
            <p> Consider two speakers, speaker A and speaker B. With the idea that MFCCs for each speaker follow a unique distribution, the Mahalanobis Distance can be utilized to classify MFCC vectors as either belonging to speaker A or speaker B. Given databases of MFCC vectors for both speakers A and B, the covariance matrices and mean vectors can be calculated for each database. Then, given an MFCC vector from the input speech signal, one can determine that this MFCC vector most likely belongs to the database with which it has a lesser Mahalanobis distance. Because each MFCC vector is calculated during a unique interval of time, this classification method allows one to determine which speaker is speaking in each corresponding interval of time. Namely, whichever speaker has the lesser Mahalanobis distance with respect to this MFCC vector can be classified as speaking in the interval of time for which this MFCC vector was calculated. This classifier was implemented, and the results were better than that of the Kth Nearest Neighbor classifier. The next section discusses the implementation of the Mahalanobis classifier.</p>

            <h5><b>Implementation</b></h5>
            <p>The implementation for the Mahalanobis classifier is very efficient. In addition to being pedagocially effective, the Cholesky Decomposition is known to be incredibly fast to compute. The Mahalanobis classifier is much faster than the Kth Nearest Neighbor classifier, taking a meager 18$\mu$s to determine the speaker versus the 34ms that the KNN algorithm takes. The determineSpeaker function simply takes in the covariance matrices and mean vectors for each speaker's database and an MFCC vector from the input signal. Then it calculates the Mahalanobis distance of the input MFCC vector with respect to both databases using Cholesky Decomposition, and finally returns the identity of the speaker that has the lesser Mahalanobis distance.</p>

        </p>
        <br><br>
        <font size="0.5"> $^1$ For those interested, this orthonormal basis is actually the set of all eigenvectors of the covariance matrix $\mathbf{S}$, each divided by its respective eigenvalue. Additionally, $\mathbf{L}$ has the same eigenvectors as $\mathbf{S}$, with each eigenvalue being the square root of the respective eigenvalue of $\mathbf{S}$.
        </font>

    </div>

    <!-- Results  -->
    <div class="container">
        <a name="results"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Experimental Results </h2>
        <p> Here we present the raw results of the $ K^{th} $ Nearest Neighbors classifier and Mahalanobis classifier, in terms of speaker classification per 10 ms or $ \frac{1}{100} $ second. It is clear that some type of post-processing is necessary to account for this noise. </p>
        <br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/raw_mahal.png">
                        <img src="pics/raw_mahal.png" alt="">
                    </a>
                    <h4> Raw Mahalanobis classifer output</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/raw_kth.png">
                        <img src="pics/raw_kth.png" alt="">
                    </a>
                    <h4> Raw $K^{th}$ Nearest Neighbors output</h4>
                </div>
            </div>
        </div>

        <p> Here we present the confusion matrices before error correction. In order to generate the confusion matrices, the classifier outputs from four different algorithm permutations were all compared to a "correct" vector that was generated manually for the sample. From these matrices, you can see that the methods perform rather poorly. The Mahalanobis classifier with MFCCs performs slightly better than the others with a dismal 61.3% accuracy. As mentioned before, post-processing of the raw classifier results is required to extract any sort of meaning from the data.</p><br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_kth_pre.png">
                        <img src="pics/mfcc_kth_pre.png" alt="">
                    </a>
                    <h4> MFCC Features using $K^{th}$ Nearest Neighbor Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_mahal_pre.png">
                        <img src="pics/mfcc_mahal_pre.png" alt="">
                    </a>
                    <h4> MFCC Features using Mahalanobis Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_env_kth_pre.png">
                        <img src="pics/mfcc_env_kth_pre.png" alt="">
                    </a>
                    <h4> MFCC and Envelope Features using $K^{th}$ Nearest Neighbor Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_env_mahal_pre.png">
                        <img src="pics/mfcc_env_mahal_pre.png" alt="">
                    </a>
                    <h4> MFCC and Envelope Features using Mahalanobis Classifier</h4>
                </div>
            </div>
        </div>

        <a name="errorcorrection"></a>
        <br>
        <h3> Error Correction </h3>
        <h4> 1) Remove Effect of Silence </h4>
        <p> One of the major sources of error for our classifier is the presence of silence in recordings. Silence is common in human speech due to breaths and pauses. If the recording is silent then the classifier cannot determine who is speaking (because nobody is). Our method determined that silence was present if the signal amplitude was within 0.2 standard deviations of the signal mean. The samples determined as silent samples were then set to have value 0. The thresholded speech signal can be seen below. It is important to note that low amplitude signal doesn't necessarily indicate that there is silence in the speech signal because the signal oscillates around its mean. Thus, a method was required to determine the presence of a significant amount of consecutive low-amplitude samples. To accomplish this, a 10ms length moving average was applied to the thresholded speech signal. If this filter output was 0 for any point in time, then the surrounding points for 5 ms in each direction were also zero. Thus, it is reasonable to assume that the speech signal is low amplitude for a significant amount of time (10 ms) whenever the filter output is 0. The plot of the moving average filter output can be seen below. Finally, all MFCC vectors calculated during a silence occured were set to speaker 1.5. This is an arbitary marker: another moving average filter applied to the silence corrected speaker classification data simply doesn't count classifications of 1.5. This filter is discussed in below in section 2) Isolated 
        Misclassifcations. </p><br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/orig_signal.png">
                        <img src="pics/orig_signal.png" alt="">
                    </a>
                    <h4> Original Speech Signal versus Time</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/thd_signal.png">
                        <img src="pics/thd_signal.png" alt="">
                    </a>
                    <h4> Thresholded Speech Signal versus Time</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/avg_signal.png">
                        <img src="pics/avg_signal.png" alt="">
                    </a>
                    <h4> Averaging Filter Output versus Time</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/breath_corr.png">
                        <img src="pics/breath_corr.png" alt="">
                    </a>
                    <h4> Mahalanobis Classifier Output with Breath Correction versus Time </h4>
                </div>
            </div>
        </div>

        <br>
        <h4> 2) Isolated Misclassifications </h4>
        <p> Even after accounting for silences and breaths, it is evident that the classifier output, displayed in the last plot above, still needs work. During the analysis of this data, we noticed that the density of correct classifications per unit time was far greater than the density of misclassifications per unit time. We call these isolated misclassifications, an example of which is a sequence of classifications $[ 2, 2, 2, 1, 2, 2] $ that we know should be $[ 2, 2, 2, 2, 2, 2]$. To resolve this issue, we took at 2.2 second long causal moving average of the surrounding classifications and removed the effect of silent portions (marked as speaker 1.5) when calculating the mean using the method described in 1) Remove Effect of Silence. This filter had the effect of measuring the density of classification points, which was our goal. If the filter output was greater than 1.5, then the algorithm determined that speaker 2 was talking, and likewise if the filter was less than 1.5 for speaker 1. The filter output is plotted below, along with a constant line at 1.5.</p> 

        <p>Another interesting property of the filter was a local max/min when the classifier predicted that a new speaker begins talking. Of course, a 2.2 second moving average requires more than 1.1 seconds of speaker classifications of the correct speaker to exist within the average. Thus, the resolution of this filter is 1.1 seconds, and a 1.1 second delay on speaker classification will occur when the algorithm estimates a change in speaker. </p><br>

                    <a href="pics/speaker_avg.png">
                        <img src="pics/speaker_avg.png" alt="">
                    </a>
  

        <h3> Final Results (after Error Correction) </h3>
        <p> Below are confusion matrices for each permutation of classifier and features used for the classifier data after these proposed corrections were made. </p>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_kth_post.png">
                        <img src="pics/mfcc_kth_post.png" alt="">
                    </a>
                    <h4> MFCC Features using $K^{th}$ Nearest Neighbor Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_mahal_post.png">
                        <img src="pics/mfcc_mahal_post.png" alt="">
                    </a>
                    <h4> MFCC Features using Mahalanobis Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_env_kth_post.png">
                        <img src="pics/mfcc_env_kth_post.png" alt="">
                    </a>
                    <h4> MFCC and Envelope Features using $K^{th}$ Nearest Neighbor Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_env_mahal_post.png">
                        <img src="pics/mfcc_env_mahal_post.png" alt="">
                    </a>
                    <h4> MFCC and Envelope Features using Mahalanobis Classifier</h4>
                </div>
            </div>
        </div>

        <p> As with before the corrections, the Mahalanobis classifier using only MFCCs has the highest rate of accuracy; however, its accuracy has increased from 61.3% to an astounding 94.8%. As mentioned in the Technical Work section, the Mahalanobis classifier was also much faster than the %K^{th}$ Nearest Neighbor classifier. These results show that the Mahalanobis classifier is clearly the best choice of classifier given our approach. The reason the Mahalanobis classifier isn't 100% accurate is mostly due to the 1.1 second resolution of the moving average filter. The algorithm has approximately 1.1 seconds of lag before it determines a new speaker is talking, as discussed in 2). Additional errors come from the fact that the MFCCs are likely not perfectly described by an N-variate Gaussian distribution as the Mahalanobis Distance assumes. Due to this, high densities of misclassifications that were not corrected by the methods proposed in 1) and 2) still may remain in the data simply due to incorrect assumptions made by the Mahalanobis classifier. These errors by misassumption comprise a very small portion of the errors.</p>
        <p>The plot below shows a comparison of the correct speaker classification (red x) vs time and the algorithm estimate for the speaker classification (blue o) vs time. Clearly, most of the discrepancies occur when the speakers change. There is a period where the algorithm estimate lags behind the correct classification due to the 1.1 second lag from the moving average as discribed before. Additionally, sparsely distributed misclassifications are also observed. This is likely due to errors by misassumption, also described previously.</p>
         <p>To correct the time lag error, efforts to account for this lag could be taken. An example of such a method is accomplished by determining points in time where the speaker changes, and then retroactively correcting the preceding 1.1 seconds of datapoints. Of course, this would take testing and time to implement because the point at which the speakers change is not set in stone due to the errors by misassumption. Given the time left to complete the project, we have omitted such a correction as the current 1.1 second resolution results are good enough for a practical speaker diarizer.</p>
        <a href="pics/SpeakerandCorrect.png">
            <img src="pics/SpeakerandCorrect.png" alt="">
        </a>
        <!-- Conclusions  -->
        <div class="container">
            <a name="conclusions"></a>
            <div class="clearfix"></div>
            <br><br>
            <hr class="featurette-divider">
            <h2> Conclusions </h2>
            <p>In conclusion, the Mahalanobis classifier using only MFCCs for features proved to be the most effective speech diarizer, both in accuracy and computation time. The Mahalanobis classifier was approximately 1900 times faster than the Kth Nearest Neighbor classifier, and at a minimum it was 3.4% more accurate. The final algorithm yielded accuracies around 95%, with most of its errors coming from the time delay due to the 2.2 second moving average filter.</p>
            <p>After our presentation, attempts to create a 3 person classifier were made; however, these attempts were unsuccesful as implementing a third speaker would require a completely different classification scheme. It is possible that the Mahalanobis distance could still be utilized, but assigning speakers to 1, 2, and 3 using moving averages clearly wouldn't work. Due to the limited time to implement such a large task, we decided to cease development of the 3 person classifier and focus our efforts elsewhere.</p>
            <p>The main goal of the project was accomplished with a high degree of accuracy. Additional goals, such as implementing features other than MFCCs, were also accomplished via the implementation of envelopes. Although it yielded lesser results than just MFCCs, this implementation was completed. It is possible that the results with envelopes were worse due to the fact that envelopes don't follow a multivariate Gaussian as the MFCCs seem to. Another goal was the development of a simple diarizer using the classification output. This goal was accomplished, and it effectively separates the multiple-speaker input .wav file into separate files, each containing only one speaker talking as seen in the Demonstration section below.</p>
            <p>From this project, we learned how to apply topics covered in class such as moving average filters, hilbert spaces, inner products, and $K^{th}$ Nearest Neighbor classification to real world problems. Additionally, we read about and learned a lot of exciting new tools such as signal envelopes, MFCCs, generalized statistical distances, and useful  matrix factorizations. This was a very exciting project to take part in, and we learned a lot in doing so</p>

        </div>

        <!-- Demo  -->
        <div class="container">
            <a name="demo"></a>
            <div class="clearfix"></div>
            <br><br>
            <hr class="featurette-divider">
            <h2> Demonstration </h2>
            <p> In order to run our demonstration first clone this 
            <a href="https://github.com/srohrer32/eecs351project"> repo </a>. Next navigate to the 
            repository in MATLAB and run <b>driver</b>. This diarizes a sample into two distinct 
            files using MFCCs and Mahalanobis distance. The resulting plot should look like this:</p>
            <a href="pics/demo.png">
               <img src="pics/demo.png" alt="">
            </a>
            <p> And the resulting sounds files should be:</p>
            <h4> Both Speakers </h4>
            <audio src="sound/bothspeaker.wav" preload="auto" controls></audio>
            <h4> Speaker 1 Only </h4>
            <audio src="sound/speaker1.wav" preload="auto" controls></audio>
            <h4> Speaker 2 Only </h4>
            <audio src="sound/speaker2.wav" preload="auto" controls></audio>
        </div>

        <!-- References  -->
        <div class="container">
            <a name="references"></a>
            <div class="clearfix"></div>
            <br><br>
            <hr class="featurette-divider">
            <h2> References </h2>
            <p> [1] M. Yankayis, "Feature Extraction Mel Frequency Cepstral Coefficients (MFFCs)," in YTU. Computer Engineering Department Website, 16-Nov-2016. </p><br>
            <p>[2] P. C. Mahalanobis, "On the generalized distance in statistics," Proc. Natl. Inst. Sci. India, vol. 2, no. 1, pp. 49-55, 1936. </p><br>
            <p>[3] K. Wojcicki, HTK MFCC MATLAB (Version 1.2). Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/</p><br>
            <p>[4] K. Wojcicki, Triangular Filterbank (Version 1.1). Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/ </p><br>
            <p>[5] K. Wojcicki, Framing Routines. Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/ </p>
        </div>

        <!-- ***************** Footer ********************** -->
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <footer>
            <p class="pull-right"><a href="#top">Back to top</a></p>
            <p>2016. Website built using Twitter Bootstrap.</p>
        </footer>
    </div>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="website/js/bootstrap.min.js"></script>

    </body>

</html>
