<!DOCTYPE html>
<html>

<head>
    <title>EECS 351 Project</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <style type="text/css">
        body {
            padding-left: -40px;
            padding-right: -40px;
            padding-top: -100px;
        }
        
        .carousel img {
            position: absolute;
            top: 0;
            left: 0;
            min-width: 100%;
            height: 500px;
        }
        
        .carousel-caption {
            margin-botton: 100px;
            text-align: center;
        }
    </style>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } });
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js"></script>
</head>


<!-- ********************** Navbar ******************* -->
<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
        <button class="navbar-toggle" data-toggle="collapse" data-target=".navHeaderCollapse">
	      <span class="sr-only">Toggle navigation</span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	    </button>
        <a class="navbar-brand" href="#">Home</a>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="navbar-collapse collapse navHeaderCollapse">
            <ul class="nav navbar-nav">
                <li class=""><a href="#intro">Introduction</a></li>
                <li class=""><a href="#data">Data</a></li>
                <li class=""><a href="#algorithm">Alogorithm Overview</a></li>
                <li class=""><a href="#technical">Technical Work</a></li>
                <li class=""><a href="#results">Experimental Results</a></li>
                <li class=""><a href="#conclusions">Conclusions</a></li>
                <li class=""><a href="#references">References</a></li>
                <li class=""><a href="progress1.html">Progress Report</a></li>
            </ul>
        </div>
    </div>
    <!-- /.navbar-collapse -->
</div>

<!-- *************** Body ***************** -->

<div class="jumbotron">
    <div class="container">
        <br><br><br><br><br><br><br><br><br><br>
        <h1>
            <center> Speaker Diarization</center>
        </h1>
        <br><br>
        <h2>
            <center>Robert Malinas and Samuel Rohrer</center>
        </h2>
        <br>
        <h3>
            <center>December 12, 2016</center>
        </h3>
        <br><br>
    </div>
</div>


<div class="container">

    <!-- Introduction  -->
    <div class="container">
        <a name="intro"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Introduction </h2>
        <p>We are Sam Rohrer and Bob Malinas. This website is dedicated to presenting our final EECS 351 project at the University of Michigan. Sam is a computer engineering major, and Bob an electrical engineering major.</p>
        <p> The goal of our project is to take a sample of human speech and determine who is speaking as a function of time. This is known as speaker diarization. Applications for speaker diarization include separating a recording of a business meeting into segments, where each segment is a specific person giving their portion of the meeting. This is to make the lives of people who have to listen to these meetings easier.  </p>
        <p> We aim to do this using Mel Frequency Cepstrum Coefficients (MFCCs), which are abstract feature vectors that are calculated using digital methods that mimic the inherent audio filtering of the human ear. Just as humans can determine who is speaking given a speech recording, the belief is that a digital process will be able to do the same. </p>
        <p> To accomplish our goal, we tested a variety of different speech features and classification methods. The method that produced the most accurate results was a classifier that used Mahalanobis distance and MFCCs. The specifics of these can be found in the section labeled Technical Work. An extensive overview of the output of each combination of features and classification methods can be viewed in the Experimental Results section.</p>
        <p> The results of our Mahalanobis classifier are very favorable, yielding highly accurate speaker classification within a resolution of one second. This means that, in any given second, the algorithm could be predicting an incorrect speaker; however, it will be correct in the next second. For long speech inputs, one second resolution is realistically good enough. Additionally, the algorithm can even work with segments of speech that have a significant amount of breaths in them. Breaths proved impossible to classify, so handling them involved some error correction. The specific methods of error correction are discussed in the Experimental Results section.</p>
    </div>

    <!-- Data  -->
    <div class="container">
        <a name="data"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Data </h2>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/onespeakerhamming.png">
                        <img src="pics/onespeakerhamming.png" alt="">
                    </a>
                    <h4> One Speaker Hamming Window Spectrogram</h4>
                    <p> This is the spectrogram of a single speaker (~12 seconds long). It was computed with a Hamming window. We hypothesize the areas around 2 seconds and 11 seconds are fricative sounds based on their spread-spectrum frequency response. The areas around 8 seconds and 10 seconds are likely plosive sounds, as they are also spread spectrum but shorter in length. </p>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/threespeakerhamming.png">
                        <img src="pics/threespeakerhamming.png" alt="">
                    </a>
                    <h4>Three Speaker Hamming Window Spectrogram</h4>
                    <p> This is the spectogram of three speakers (~18 seconds long) computed with a Hamming window. For this plot we hypothesize that areas around 5 seconds and 13 seconds are fricative sounds based on their spread-spectrum frequency response. The areas around 9 seconds and 10 seconds are probably plosive sounds as they are spread spectrum but shorter in length. </p>
                </div>
            </div>

            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_plot.png">
                        <img src="pics/mfcc_plot.png" alt="">
                    </a>
                    <h4>MFC Coefficients Plot</h4>
                    <p> This is the plot of a single sample (30 ms) computed with a Hamming window. The signal was band-limited at 300 Hz and 5000 Hz, based on the frequency spectrum of typical speech. It was then binned into 25 mel frequency bins. The plot shown here is the result of the MFCC computation, with the x-axis as dimension and the y-axis as magnitude. </p>
                </div>
            </div>

        </div>
    </div>

    <!-- Algorithms  -->
    <div class="container">
        <a name="algorithm"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Algorithm Overview </h2>
        <div class="thumbnail">
            <a href="pics/pipeline.png">
                <img src="pics/pipeline.png" alt="">
            </a>
            <h4> Pipeline View of Algorithms Used</h4>
            <p> <a href="#envelopes"> Features: envelopes </a></p>
            <p> <a href="#mfcc"> Features: Mel Frequency Cepstrum Coefficients (MFCCs) </a></p>
            <p> <a href="#knn"> Classification: Kth Nearest Neighbors </a></p>
            <p> <a href="#mahalanobis"> Classification: Mahalanobis Distance </a></p>
            <p> <a href="#errorcorrection"> Error Correction </a></p>

        </div>
    </div>

    <!-- Technical Work  -->
    <div class="container">
        <a name="technical"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Technical Work </h2>
        <p> In this section we present the technical work conducted throughout the semester while trying to create a speaker diarization system. There are two sections, audio features and speaker classification. In audio features we discuss envelopes and MFCCs. In speaker classification, we discuss the $K^{th}$ nearest neighbors method and the Mahalanobis distance method. Note that not all algorithms discussed were used in the final speaker diarization system.
            <a name="envelopes"></a>
            <br>
            <h3> Audio Features </h3>
            <br>
            <h4> Envelopes </h4>
            <p> Based on reference [1], we know that peaks in the spectral envelope of the time domain signal are known as formants. Additionally, different speakers have different formants and different sounds also have different formants. We first computed the envelope of the absolute value of the signal. In order to identify the formants, we compute the energy under the envelope at six different thresholds. The thresholds include less and less of the signal to help identify the formants. The features used were: $$ \begin{equation} e_1 = trapz(envelope(envelope > thd)), thd = \frac{1}{2} \end{equation} $$ $$ \begin{equation} e_2 = trapz(envelope(envelope > thd)), thd = \frac{5}{9} \end{equation}$$ $$ \begin{equation} e_3 = trapz(envelope(envelope > thd)), thd = \frac{5}{8} \end{equation}$$ $$ \begin{equation} e_4 = trapz(envelope(envelope > thd)), thd = \frac{2}{3} \end{equation}$$ $$ \begin{equation} e_5 = trapz(envelope(envelope > thd)), thd = \frac{3}{4} \end{equation}$$ $$ \begin{equation} e_6 = trapz(envelope(envelope > thd)), thd = \frac{7}{8} \end{equation}$$ </p><br>

            <div class="row">
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/formant.png">
                            <img src="pics/formant.png" alt="">
                        </a>
                        <h4> Formants Example</h4>
                        <p> This diagram shows an example of speech formants, taken from
                            <a href="http://person2.sol.lu.se/SidneyWood/praate/images/formant03.gif">
                    link.</a></p>
                    </div>
                </div>
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/envelopeSample.png">
                            <img src="pics/envelopeSample.png" alt="">
                        </a>
                        <h4>Sample Envelope Coefficients</h4>
                        <p> These are sample envelope coefficients from Bob (blue) and Sam (red), you can see the difference between the two that should allow succesful speaker classification. </p>
                    </div>
                </div>
            </div>

            <a name="mfcc"></a>
            <br>
            <h4> Mel Frequency Cepstrum Coefficients (MFCCs) </h4>
            <p> Here we outline the steps required to generate MFCCs. MFCCs measure the short term log power spectrum of the signal. This is based on the Mel scale (see image below), which separates the frequency spectrum into non-linear bins which more closely mimic human hearing. The Mel scale achieves this with smaller bins (~100 Hz wide) at lower frequencies and larger bins (~1000 Hz wide) at higher frequencies.</p><br>

            <p> 1) Window the signal into 30 ms intervals with a 20 ms overlap, which means that there are 10 ms of new signal information for every window. Then a Hamming window was applied to prevent spectral leakage following Equation 7 ($N=30$, $n=[1,30]$): $$ \begin{equation} W(n) = 0.54 + 0.46*cos(\frac{2\pi n}{N-1}) \end{equation} $$ </p> <br>
            <p> 2) Compute the Fast Fourier Transform (FFT) of the windowed signal </p> <br>
            <p> 3) Compute the magnitude of the FFT coefficients: $$ \begin{equation} FFT_{mag} = |FFT|^2 \end{equation}$$ </p><br>
            <p> 4) Choose $f_{max} = 3000 Hz$, $f_{min} = 100 Hz$, and the number of Mel bins $Bins_{Mel} = 15$. Compute the Mel scale: $$\begin{equation} f_{max} Mels = 1125 * log(1 + \frac{f_{max} Hz}{700})\end{equation}$$ $$\begin{equation} f_{min} Mels = 1125 * log(1 + \frac{f_{min} Hz}{700})\end{equation}$$ </p><br>
            <p> 5) Using $f_{max}$ and $f_{min}$ in Mels, and the number of Mel bins create equispaced filters on the Mel scale. Next convert these Mel frequencies back to Hz to determine the location of the filters: $$\begin{equation} f_{filter} Hz = 700 * (e^{\frac{f_{filter} Mel}{1125}} - 1) \end{equation}$$ This creates a Mel filter bank similar to the image, "Sample Mel Filter Bank".</p><br>
            <p> 6) Compute the magnitude of $FFT_{mag}$ within each of the triangular Mel filter banks. </p><br>
            <p> 7) Compute the log of these magnitudes. Length of vector here should be equal to the number of Mel bins. </p><br>
            <p> 8) Generate the MFCCs by computing the Inverse FFT of the log of magnitudes. In this case the IFFT reduces to the Discrete Cosine Transform (DCT). The result should be similar to the image "Sample MFCCs".</p><br>

            <div class="row">
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/melfilterbank.png">
                            <img src="pics/melfilterbank.png" alt="">
                        </a>
                        <h4> Sample Mel Filter Bank</h4>
                        <p> This diagram shows a sample mel filter bank, taken from
                            <a href="https://www.vocal.com/wp-content/uploads/2013/06/MelFilterBank.jpeg">
                    link.</a></p>
                    </div>
                </div>
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/mfccs.png">
                            <img src="pics/mfccs.png" alt="">
                        </a>
                        <h4>Sample MFCCs</h4>
                        <p> These are sample MFCCs from Bob (blue) and Sam (red), you can see the difference between the two that should allow succesful speaker classification. This was computed for 13 filter banks. </p>
                    </div>
                </div>
            </div>

            <br>
            <h3> Speaker Classification </h3>
            <a name="knn"></a>
            <br>
            <h4> Kth Nearest Neighbors </h4>
            <p>In this section, the Kth Nearest Neighbors classification algorithm is discussed. This algorithm is very simple in complexity, however its lack of elegance is evident during implementation. It is a very brute force method of classification, and its results are not as good as those for the other classifier we developed. Additionally, it takes far longer to run. The exact runtimes are discussed in the Implementation subsection of the Mahalanobis Distance section.
            </p>

            <p>The idea behind Kth Nearest Neighbors is to find, quite literally, the nearest k vectors in an N-dimensional space using the Euclidean metric. In this case, these N-dimensional vectors are MFCC vectors. In this application, MFCC databases of each speaker are generated. That is, given a speech segment with only one of the speakers talking, a matrix whose columns are MFCC vectors for each time interval is generated. Then, given an input speech vector with these two speakers talking at different times, MFCC vectors are generated. Given a single input MFCC vector, the Kth Nearest Neighbors algorithm compares this single MFCC vector with all of the vectors in both databases. Specifically, the Euclidean distance between the single input vector and every vector in both databases is calculated. Then, the database vectors with the k smallest distances are recorded. The speaker whose vectors comprise the majority of these k vectors is determined as who is speaking for the time interval from which this MFCC vector was generated. In our experiments with the Kth Nearest Neighbors algorithm we tested several different values of k. We found that, no matter the value of k, the Mahalnobis classifier was much faster and more accurate, although sometimes marginally so.</p>
            <p>Below is a diagram demonstrating the Kth Nearest Neighbors method for 3 different values of k. The red and green points represent 2-dimensional MFCC vectors for example red and green speakers, respectively. The blue vector is the single input MFCC vector that is being compared. In this example, the blue point would be classified as the red speaker talking for all three values of k. The axes for this plot are simply contrived example MFCCs.</p>
            <br>
           
                        <a href="pics/kthnearestexample.png">
                            <img src="pics/kthnearestexample.png" alt="">
                        </a>
            <br>
            <a name="mahalanobis"></a>
            <br>
            <h4> Mahalanobis Distance </h4>
            <p>
                <h5><b>Introduction</b></h5>
                In this section, the Mahalanobis Distance and its utilization with MFCCs are discussed. Suppose the MFFCs for each speaker are distributed in an N-dimensional space, and there exists a unique distribution for each speaker. Then, it would be useful to determine how likely it is that an MFCC vector belongs to a specific speaker's distribution. During development, this idea was tested and confirmed. The next section presents the mathematics necessary to understand the Mahalanobis classifier.</p>

            <h5><b>Mathematics and Explanation</b></h5>
            <p>Consider a real N-dimensional column vector $ \mathbf{\vec{v}} $. Then, let $ \mathbf{S} $ be the covariance matrix of a real N-dimensional distribution and let $\boldsymbol{\vec{\mu}}$ be a vector containing the mean along each dimension of this distribution. The Mahalanobis Distance is defined as $$ \begin{equation} D_{M} = \sqrt{(\mathbf{\vec{v}} - \boldsymbol{\vec{\mu}})^{\,T}\mathbf{S}^{-1}(\mathbf{\vec{v}} - \boldsymbol{\vec{\mu}})}. \end{equation}$$ Let $$ \begin{equation} \mathbf{\vec{d}} = \mathbf{\vec{v}} - \boldsymbol{\vec{\mu}}. \end{equation}$$ Note that $\mathbf{\vec{d}}$ is simply the vector difference between the vector being compared to the distribution, $\mathbf{\vec{v}}$, and the mean vector, $\boldsymbol{\vec{\mu}}$. The importance of $\mathbf{\vec{d}}$ will become clear shortly. It is difficult to understand the Mahalanobis Distance from (12). To develop intuition, consider the relation $$ \begin{equation} \mathbf{S} = \mathbf{LL}^T, \end{equation}$$ where $\mathbf{L}$ is the Cholesky Decomposition of $\mathbf{S}$. This decomposition is guaranteed to exist because $\mathbf{S}$, being a covariance matrix, is positive semi-definite. After combining (12), (13), and (14), the result reduces to $$\begin{equation} D_{M} = \sqrt{
                <\mathbf{L^{-1}\vec{d}}, \mathbf{L^{-1}\vec{d}}>}, \end{equation}$$ which is simply the 2-norm of the vector $\mathbf{L^{-1}\vec{d}}$. Another important relation will clarify why the Mahalanobis distance works. Consider a completely uncorrelated N-dimensional random column vector $\mathbf{X}$ that has unit variance along each dimension. Let $$\begin{equation} \mathbf{I} = cov(\mathbf{X}). \end{equation}$$ The symbol $\mathbf{I}$ was chosen intentionally, as this covariance matrix is necessarily an N by N identity matrix. Now, suppose $\mathbf{X}$ is linearly transformed by a matrix $\mathbf{L}$. Again, the choice of the symbol $\mathbf{L}$ for the transformation matrix is intentional. Then, $$\begin{equation} cov(\mathbf{LX}) = \mathbf{L}cov(\mathbf{X})\mathbf{L}^T, \end{equation}$$ but (16) shows that $cov(\mathbf{X})$ is an identity matrix. Therefore, (17) becomes $$\begin{equation} cov(\mathbf{LX}) = \mathbf{LL}^T = \mathbf{S}. \end{equation}$$ This profound result shows that a random sample distribution of N-dimensional vectors with covariance matrix $ \mathbf{S}$ can be linearly transformed by the inverse Cholesky decomposition of $ \mathbf{S} $, namely $\mathbf{L^{-1}}$, to become an uncorrelated distribution that has unit variance along each dimension. This completely removes the bias caused by the distribution from each sample vector. Then, it is clear that the Mahalanobis Distance is simply 2-norm of the difference vector $\mathbf{\vec{d}}$ when viewed in the context of an orthonormal basis$^1$ that is chosen carefully to remove the bias of the distribution. For a less rigorous, intuitive description of the Mahalanobis distance, continue reading.</p>

            <h5><b>Intuitive Description</b></h5>
            <p> The Mahalanobis Distance is an N-dimensional analogue to the z-score, which tells how many standard deviations a point is away from the mean in a Gaussian distribution. The z-score of a sample point with respect to a distribution allows one to determine how likely it is that this sample point belongs to the distribution. If two z-scores are calculated for the sample point, each with respect to a unique distribution, then the point most likely belongs to the distribution with which it has the lesser z-score. The same principle can be applied using the Mahalanobis Distance for N-variate distributions. Given an N-dimensional sample vector and two distributions, the sample vector most likely belongs to the distribution with which it has a lesser Mahalanobis distance. Another way to view this is that the sample vector is "less of an outlier" in the distribution where its Mahalanobis distance is the smallest. The next section describes the Mahalanobis classifier.
            </p>

            <h5><b>Method of Classification</b></h5>
            <p> Consider two speakers, speaker A and speaker B. With the idea that MFCCs for each speaker follow a unique distribution, the Mahalanobis Distance can be utilized to classify MFCC vectors as either belonging to speaker A or speaker B. Given databases of MFCC vectors for both speakers A and B, the covariance matrices and mean vectors can be calculated for each database. Then, given an MFCC vector from the input speech signal, one can determine that this MFCC vector most likely belongs to the database with which it has a lesser Mahalanobis distance. Because each MFCC vector is calculated during a unique interval of time, this classification method allows one to determine which speaker is speaking in each corresponding interval of time. Namely, whichever speaker has the lesser Mahalanobis distance with respect to this MFCC vector can be classified as speaking in the interval of time for which this MFCC vector was calculated. This classifier was implemented, and the results were better than that of the Kth Nearest Neighbor classifier. The next section discusses the implementation of the Mahalanobis classifier.</p>

            <h5><b>Implementation</b></h5>
            <p>The implementation for the Mahalanobis classifier is very efficient. In addition to being pedagocially effective, the Cholesky Decomposition is known to be incredibly fast to compute. The Mahalanobis classifier is much faster than the Kth Nearest Neighbor classifier, taking a meager 18$\mu$s to determine the speaker versus the 34ms that the KNN algorithm takes. The determineSpeaker function simply takes in the covariance matrices and mean vectors for each speaker's database and an MFCC vector from the input signal. Then it calculates the Mahalanobis distance of the input MFCC vector with respect to both databases using Cholesky Decomposition, and finally returns the identity of the speaker that has the lesser Mahalanobis distance.</p>

        </p>
        <br><br>
        <font size="0.5"> $^1$ For those interested, this orthonormal basis is actually the set of all eigenvectors of the covariance matrix $\mathbf{S}$, each divided by its respective eigenvalue. Additionally, $\mathbf{L}$ has the same eigenvectors as $\mathbf{S}$, with each eigenvalue being the square root of the respective eigenvalue of $\mathbf{S}$.
        </font>

    </div>

    <!-- Results  -->
    <div class="container">
        <a name="results"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Experimental Results </h2>
        <p> Here we present the raw results of the $ K^{th} $ nearest neighbors classifier and Mahalanobis classifier, in terms of speaker classification per 10 ms or $ \frac{1}{100} $ second. It is clear that some type of post-processing is necessary to account for this noise. </p>
        <br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/raw_mahal.png">
                        <img src="pics/raw_mahal.png" alt="">
                    </a>
                    <h4> Raw Mahalanobis classifer output</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/raw_kth.png">
                        <img src="pics/raw_kth.png" alt="">
                    </a>
                    <h4> Raw $K^{th}$ nearest neighbors output</h4>
                </div>
            </div>
        </div>

        <p> Here we present the confusion matrices before error correction. In order to generate the confusion matrices the classifier outputs from the four different algorithm permutations were all compared to a "correct" vector that was generated manually for the sample. </p><br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_kth_pre.png">
                        <img src="pics/mfcc_kth_pre.png" alt="">
                    </a>
                    <h4> MFCC Features using $K^{th}$ Nearest Neighbor Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_mahal_pre.png">
                        <img src="pics/mfcc_mahal_pre.png" alt="">
                    </a>
                    <h4> MFCC Features using Mahalanobis Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_env_kth_pre.png">
                        <img src="pics/mfcc_env_kth_pre.png" alt="">
                    </a>
                    <h4> MFCC and Envelope Features using $K^{th}$ Nearest Neighbor Classifier</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_env_mahal_pre.png">
                        <img src="pics/mfcc_env_mahal_pre.png" alt="">
                    </a>
                    <h4> MFCC and Envelope Features using Mahalanobis Classifier</h4>
                </div>
            </div>
        </div>

        <a name="errorcorrection"></a>
        <br>
        <h3> Error Correction </h3>
        <h4> 1) Remove Effect of Silence </h4>
        <p> One of the major problems with our classifier is the presence of silence in recordings. If the recording is silent then the classifier cannot determine the speaker (because there isn't one). We said that silence was present if the absolute value of the signal was within 0.2 standard deviations of the signal mean. These samples were then set to 0. To further smooth the signal after this a 10 ms long averaging filter was applied with no overlap. Finally, all sections where a silence occured were set to 1.5, a value impossible to have a speaker at with speaker 1 and speaker 2. </p><br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/orig_signal.png">
                        <img src="pics/orig_signal.png" alt="">
                    </a>
                    <h4> Original Speech Signal versus Time</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/thd_signal.png">
                        <img src="pics/thd_signal.png" alt="">
                    </a>
                    <h4> Thresholded Speech Signal versus Time</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/avg_signal.png">
                        <img src="pics/avg_signal.png" alt="">
                    </a>
                    <h4> Averaging Filter Output versus Time</h4>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/breath_corr.png">
                        <img src="pics/breath_corr.png" alt="">
                    </a>
                    <h4> Mahalanobis Classifier Output with Breath Correction versus Time </h4>
                </div>
            </div>
        </div>

        <br>
        <h4> 2) Isolated Misclassifications </h4>
        <p> Another problem with our classifier was isolated misclassifications, an example of this is a sequence of classifications $[ 2, 2, 2, 1, 2, 2] $ that we know should be $[ 2, 2, 2, 2, 2, 2]$. To solve this we took at 2.2 second causal moving average of previous classifications, and removed the effect of silent portions when calculating the mean. This filter had the effect of measuring the density of classification points, which was our goal. Another interesting property of the filter was a local max/min when a new speaker begins talking.</p><br>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/speaker_avg.png">
                        <img src="pics/speaker_avg.png" alt="">
                    </a>
                    <h4> Speaker after Moving Average Filter versus Time</h4>
                </div>
            </div>
        </div>

        <!-- Conclusions  -->
        <div class="container">
            <a name="conclusions"></a>
            <div class="clearfix"></div>
            <br><br>
            <hr class="featurette-divider">
            <h2> Conclusions </h2>

        </div>

        <!-- References  -->
        <div class="container">
            <a name="references"></a>
            <div class="clearfix"></div>
            <br><br>
            <hr class="featurette-divider">
            <h2> References </h2>
            <p> [1] M. Yankayis, "Feature Extraction Mel Frequency Cepstral Coefficients (MFFCs)," in YTU. Computer Engineering Department Website, 16-Nov-2016. </p><br>
            <p>[2] P. C. Mahalanobis, "On the generalized distance in statistics," Proc. Natl. Inst. Sci. India, vol. 2, no. 1, pp. 49-55, 1936. </p><br>
            <p>[3] K. Wojcicki, HTK MFCC MATLAB (Version 1.2). Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/</p><br>
            <p>[4] K. Wojcicki, Triangular Filterbank (Version 1.1). Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/ </p><br>
            <p>[5] K. Wojcicki, Framing Routines. Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/ </p>
        </div>

        <!-- ***************** Footer ********************** -->
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <footer>
            <p class="pull-right"><a href="#top">Back to top</a></p>
            <p>2016. Website built using Twitter Bootstrap.</p>
        </footer>
    </div>

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="website/js/bootstrap.min.js"></script>

    </body>

</html>
