<!DOCTYPE html>
<html>

<head>
    <title>EECS 351 Project</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="css/bootstrap.min.css" rel="stylesheet">
    <style type="text/css">
        body {
            padding-left: -40px;
            padding-right: -40px;
            padding-top: -100px;
        }
        
        .carousel img {
            position: absolute;
            top: 0;
            left: 0;
            min-width: 100%;
            height: 500px;
        }
        
        .carousel-caption {
            margin-botton: 100px;
            text-align: center;
        }
    </style>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "AMS" } } });
    </script>
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: {inlineMath: [["$","$"],["\\(","\\)"]]} });
    </script>
    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML,http://myserver.com/MathJax/config/local/local.js"></script>
</head>


<!-- ********************** Navbar ******************* -->
<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container">
        <button class="navbar-toggle" data-toggle="collapse" data-target=".navHeaderCollapse">
	      <span class="sr-only">Toggle navigation</span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	    </button>
        <a class="navbar-brand" href="#">Home</a>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="navbar-collapse collapse navHeaderCollapse">
            <ul class="nav navbar-nav">
                <li class=""><a href="#intro">Introduction</a></li>
                <li class=""><a href="#data">Data</a></li>
                <li class=""><a href="#algorithm">Alogorithm Overview</a></li>
                <li class=""><a href="#technical">Technical Work</a></li>
                <li class=""><a href="#results">Experimental Results</a></li>
                <li class=""><a href="#conclusions">Conclusions</a></li>
                <li class=""><a href="#references">References</a></li>
                <li class=""><a href="progress1.html">Progress Report</a></li>
            </ul>
        </div>
    </div>
    <!-- /.navbar-collapse -->
</div>

<!-- *************** Body ***************** -->

<div class="jumbotron">
    <div class="container">
        <br><br><br><br><br><br><br><br><br><br>
        <h1>
            <center> Speaker Diarization</center>
        </h1>
        <br><br>
        <h2>
            <center>Robert Malinas and Samuel Rohrer</center>
        </h2>
        <br>
        <h3>
            <center>December 12, 2016</center>
        </h3>
        <br><br>
    </div>
</div>


<div class="container">

    <!-- Introduction  -->
    <div class="container">
        <a name="intro"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Introduction </h2>
        <p> Given a recording with multiple people speaking at different times (ex. meeting) identify who is speaking in each second. Then we aim to take this information and separate the signal into individual signals for each speaker, or diarize the signal. A prerequisite to this is a past database of samples of all the speakers. </p> <br>
        <p> We aim to do this using Mel Frequency Cepstrum Coefficients (MFCCs), which were originally developed in... </p>
    </div>

    <!-- Data  -->
    <div class="container">
        <a name="data"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Data </h2>
        <div class="row">
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/onespeakerhamming.png">
                        <img src="pics/onespeakerhamming.png" alt="">
                    </a>
                    <h4> One Speaker Hamming Window Spectrogram</h4>
                    <p> This is the spectrogram of a single speaker (~12 seconds long). It was computed with a Hamming window. We hypothesize the areas around 2 seconds and 11 seconds are fricative sounds based on their spread-spectrum frequency response. The areas around 8 seconds and 10 seconds are likely plosive sounds, as they are also spread spectrum but shorter in length. </p>
                </div>
            </div>
            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/threespeakerhamming.png">
                        <img src="pics/threespeakerhamming.png" alt="">
                    </a>
                    <h4>Three Speaker Hamming Window Spectrogram</h4>
                    <p> This is the spectogram of three speakers (~18 seconds long) computed with a Hamming window. For this plot we hypothesize that areas around 5 seconds and 13 seconds are fricative sounds based on their spread-spectrum frequency response. The areas around 9 seconds and 10 seconds are probably plosive sounds as they are spread spectrum but shorter in length. </p>
                </div>
            </div>

            <div class="col-3 col-lg-3">
                <div class="thumbnail">
                    <a href="pics/mfcc_plot.png">
                        <img src="pics/mfcc_plot.png" alt="">
                    </a>
                    <h4>MFC Coefficients Plot</h4>
                    <p> This is the plot of a single sample (30 ms) computed with a Hamming window. The signal was band-limited at 300 Hz and 5000 Hz, based on the frequency spectrum of typical speech. It was then binned into 25 mel frequency bins. The plot shown here is the result of the MFCC computation, with the x-axis as dimension and the y-axis as magnitude. </p>
                </div>
            </div>

        </div>
    </div>

    <!-- Algorithms  -->
    <div class="container">
        <a name="algorithm"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Algorithm Overview </h2>
        <div class="thumbnail">
            <a href="pics/pipeline.png">
                <img src="pics/pipeline.png" alt="">
            </a>
            <h4> Pipeline View of Algorithms Used</h4>
            <p> <a href="#envelopes"> Features: envelopes </a></p>
            <p> <a href="#mfcc"> Features: Mel Frequency Cepstrum Coefficients (MFCCs) </a></p>
            <p> <a href="#knn"> Classification: Kth Nearest Neighbors </a></p>
            <p> <a href="#mahalanobis"> Classification: Mahalanobis Distance </a></p>
            <p> <a href="#errorcorrection"> Error Correction </a></p>

        </div>
    </div>

    <!-- Technical Work  -->
    <div class="container">
        <a name="technical"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Technical Work </h2>
        <p> In this section we present the technical work conducted throughout the semester while trying to create a speaker diarization system. There are two sections, audio features and speaker classification. In audio features we discuss envelopes and MFCCs. In speaker classification, we discuss the kth nearest neighbors method and the Mahalanobis distance method. Note that not all algorithms discussed were used in the final speaker diarization system.
            <a name="envelopes"></a>
            <br>
            <h3> Audio Features </h3>
            <br>
            <h4> Envelopes </h4>
            <p> Based on reference [1], we know that peaks in the spectral envelope of the time domain signal are known as formants. Additionally, different speakers have different formants and different sounds also have different formants. We first computed the envelope of the absolute value of the signal. In order to identify the formants, we compute the energy under the envelope at 6 different thresholds. The thresholds include less and less of the signal to help identify the formants. The features used were: 
            $$ \begin{equation} e_1 = trapz(envelope(envelope > thd)), thd = \frac{1}{2} \end{equation} $$ 
            $$ \begin{equation} e_2 = trapz(envelope(envelope > thd)), thd = \frac{5}{9} \end{equation}$$ 
            $$ \begin{equation} e_3 = trapz(envelope(envelope > thd)), thd = \frac{5}{8} \end{equation}$$ 
            $$ \begin{equation} e_4 = trapz(envelope(envelope > thd)), thd = \frac{2}{3} \end{equation}$$ 
            $$ \begin{equation} e_5 = trapz(envelope(envelope > thd)), thd = \frac{3}{4} \end{equation}$$ 
            $$ \begin{equation} e_6 = trapz(envelope(envelope > thd)), thd = \frac{7}{8} \end{equation}$$ </p><br>

            <div class="row">
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/formant.png">
                            <img src="pics/formant.png" alt="">
                        </a>
                        <h4> Formants Example</h4>
                        <p> This diagram shows an example of speech formants, taken from
                            <a href="http://person2.sol.lu.se/SidneyWood/praate/images/formant03.gif">
                    link.</a></p>
                    </div>
                </div>
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/envelopeSample.png">
                            <img src="pics/envelopeSample.png" alt="">
                        </a>
                        <h4>Sample Envelope Coefficients</h4>
                        <p> These are sample envelope coefficients from Bob (blue) and Sam (red), you can see the difference between the two that should allow succesful speaker classification. </p>
                    </div>
                </div>
            </div>

            <a name="mfcc"></a>
            <br>
            <h4> Mel Frequency Cepstrum Coefficients (MFCCs) </h4>
            <p> Here we outline the steps required to generate MFCCs. MFCCs measure the short term log power spectrum of the signal. This is based on the Mel scale (see image below), which separates the frequency spectrum into non-linear bins which more closely mimic human hearing. The Mel scale achieves this with smaller bins (~100 Hz wide) at lower frequencies and larger bins (~1000 Hz wide) at higher frequencies.</p><br>

            <p> 1) Window the signal into 30 ms intervals with a 20 ms overlap, which means that there are 10 ms of new signal information for every window. Then a Hamming window was applied following this formula ($N=30$, $n=[1,30]$): $$ \begin{equation} W(n) = 0.54 + 0.46*cos(\frac{2\pi n}{N-1}) \end{equation} $$ </p> <br>
            <p> 2) Compute the Fast Fourier Transform (FFT) of the windowed signal </p> <br>
            <p> 3) Compute the magnitude of the FFT coefficients: $$ \begin{equation} FFT_{mag} = |FFT|^2 \end{equation}$$ </p><br>
            <p> 4) Choose $f_{max} = 3000 Hz$, $f_{min} = 100 Hz$, and the number of Mel bins $Bins_{Mel} = 15$. Compute the Mel scale: $$\begin{equation} f_{max} Mels = 1125 * log(1 + \frac{f_{max} Hz}{700})\end{equation}$$ $$\begin{equation} f_{min} Mels = 1125 * log(1 + \frac{f_{min} Hz}{700})\end{equation}$$ </p><br>
            <p> 5) Using $f_{max}$ and $f_{min}$ in Mels, and the number of Mel bins create equispaced filters on the Mel scale. Next convert these Mel frequencies back to Hz to determine the location of the filters: $$\begin{equation} f_{filter} Hz = 700 * (e^{\frac{f_{filter} Mel}{1125}} - 1) \end{equation}$$ This creates a Mel filter bank similar to the image, "Sample Mel Filter Bank".</p><br>
            <p> 6) Compute the magnitude of $FFF_{mag}$ within each of the triangular Mel filter banks. </p><br>
            <p> 7) Compute the log of these magnitudes. Length of vector here should be equal to the number of Mel bins. </p><br>
            <p> 8) Generate the MFCCs by computing the Inverse FFT of the log of magnitudes. In this case the IFFT reduces to the Discrete Cosine Transform (DCT). The result should be similar to the image "Sample MFCCs".</p><br>

            <div class="row">
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/melfilterbank.png">
                            <img src="pics/melfilterbank.png" alt="">
                        </a>
                        <h4> Sample Mel Filter Bank</h4>
                        <p> This diagram shows a sample mel filter bank, taken from
                            <a href="https://www.vocal.com/wp-content/uploads/2013/06/MelFilterBank.jpeg">
                    link.</a></p>
                    </div>
                </div>
                <div class="col-3 col-lg-3">
                    <div class="thumbnail">
                        <a href="pics/mfccs.png">
                            <img src="pics/mfccs.png" alt="">
                        </a>
                        <h4>Sample MFCCs</h4>
                        <p> These are sample MFCCs from Bob (blue) and Sam (red), you can see the difference between the two that should allow succesful speaker classification. This was computed for 13 filter banks. </p>
                    </div>
                </div>
            </div>

            <br>
            <h3> Speaker Classification </h3>
            <a name="knn"></a>
            <br>
            <h4> Kth Nearest Neighbors </h4>
            <a name="mahalanobis"></a>
            <br>
            <h4> Mahalanobis Distance </h4>
            <p>
            <h5><b>Introduction</b></h5>
            In this section, the Mahalanobis Distance and its utilization with MFCCs are discussed. Suppose the MFFCs for each speaker are distributed in an N-dimensional space, and there exists a unique distribution for each speaker. Then, it would be useful to determine how likely it is that an MFCC vector belongs to a specific speaker's distribution. During development, this idea was tested and confirmed. The next section presents the mathematics necessary to understand the Mahalanobis classifier.</p>

            <h5><b>Mathematics and Explanation</b></h5>
            <p>Consider a real N-dimensional column vector $ \mathbf{\vec{v}} $. Then, let $ \mathbf{S} $ be the covariance matrix of a real N-dimensional distribution and let $\boldsymbol{\vec{\mu}}$ be a vector containing the mean along each dimension of this distribution. The Mahalanobis Distance is defined as

            $$ \begin{equation} D_{M} = \sqrt{(\mathbf{\vec{v}} - \boldsymbol{\vec{\mu}})^{\,T}\mathbf{S}^{-1}(\mathbf{\vec{v}} - \boldsymbol{\vec{\mu}})}. \end{equation}$$ 

            Let

            $$ \begin{equation} \mathbf{\vec{d}} = \mathbf{\vec{v}} - \boldsymbol{\vec{\mu}}. \end{equation}$$

            Note that $\mathbf{\vec{d}}$ is simply the vector difference between the vector being compared to the distribution, $\mathbf{\vec{v}}$, and the mean vector, $\boldsymbol{\vec{\mu}}$. The importance of $\mathbf{\vec{d}}$ will become clear shortly. It is difficult to understand the Mahalanobis Distance from (12). To develop intuition, consider the relation
            
            $$ \begin{equation} \mathbf{S} = \mathbf{LL}^T, \end{equation}$$ 

            where $\mathbf{L}$ is the Cholesky Decomposition of $\mathbf{S}$. This decomposition is guaranteed to exist because $\mathbf{S}$, being a covariance matrix, is positive semi-definite. After combining (12), (13), and (14), the result reduces to 

            $$\begin{equation} D_{M} = \sqrt{<\mathbf{L^{-1}\vec{d}}, \mathbf{L^{-1}\vec{d}}>}, \end{equation}$$

            which is simply the 2-norm of the vector $\mathbf{L^{-1}\vec{d}}$. Another important relation will clarify why the Mahalanobis distance works. Consider a completely uncorrelated N-dimensional random column vector $\mathbf{X}$ that has unit variance along each dimension. Let 

            $$\begin{equation} \mathbf{I} = cov(\mathbf{X}). \end{equation}$$
            
             The symbol $\mathbf{I}$ was chosen intentionally, as this covariance matrix is necessarily an N by N identity matrix. Now, suppose $\mathbf{X}$ is linearly transformed by a matrix $\mathbf{L}$. Again, the choice of the symbol $\mathbf{L}$ for the transformation matrix is intentional. Then,

            $$\begin{equation} cov(\mathbf{LX}) = \mathbf{L}cov(\mathbf{X})\mathbf{L}^T, \end{equation}$$

            but (16) shows that $cov(\mathbf{X})$ is an identity matrix. Therefore, (17) becomes

            $$\begin{equation} cov(\mathbf{LX}) = \mathbf{LL}^T = \mathbf{S}. \end{equation}$$
            
            This profound result shows that a random sample distribution of N-dimensional vectors with covariance matrix $ \mathbf{S}$ can be linearly transformed by the inverse Cholesky decomposition of $ \mathbf{S} $, namely $\mathbf{L^{-1}}$, to become an uncorrelated distribution that has unit variance along each dimension. This completely removes the bias caused by the distribution from each sample vector. Then, it is clear that the Mahalanobis Distance is simply 2-norm of the difference vector $\mathbf{\vec{d}}$ when viewed in the context of an orthonormal basis$^1$ that is chosen carefully to remove the bias of the distribution. For a less rigorous, intuitive description of the Mahalanobis distance, continue reading.</p>

            <h5><b>Intuitive Description</b></h5>
            <p> The Mahalanobis Distance is an N-dimensional analogue to the z-score, which tells how many standard deviations a point is away from the mean in a Gaussian distribution. The z-score of a sample point with respect to a distribution allows one to determine how likely it is that this sample point belongs to the distribution. If two z-scores are calculated for the sample point, each with respect to a unique distribution, then the point most likely belongs to the distribution with which it has the lesser z-score. The same principle can be applied using the Mahalanobis Distance for N-variate distributions. Given an N-dimensional sample vector and two distributions, the sample vector most likely belongs to the distribution with which it has a lesser Mahalanobis distance. Another way to view this is that the sample vector is "less of an outlier" in the distribution where its Mahalanobis distance is the smallest. The next section describes the Mahalanobis classifier.
            </p>

            <h5><b>Method of Classification</b></h5>
            <p> Consider two speakers, speaker A and speaker B. With the idea that MFCCs for each speaker follow a unique distribution, the Mahalanobis Distance can be utilized to classify MFCC vectors as either belonging to speaker A or speaker B. Given databases of MFCC vectors for both speakers A and B, the covariance matrices and mean vectors can be calculated for each database. Then, given an MFCC vector from the input speech signal, one can determine that this MFCC vector most likely belongs to the database with which it has a lesser Mahalanobis distance. Because each MFCC vector is calculated during a unique interval of time, this classification method allows one to determine which speaker is speaking in each corresponding interval of time. Namely, whichever speaker has the lesser Mahalanobis distance with respect to this MFCC vector can be classified as speaking in the interval of time for which this MFCC vector was calculated. This classifier was implemented, and the results were better than that of the K-th Nearest Neighbor classifier. The next section discusses the implementation of the Mahalanobis classifier.</p>

            <h5><b>Implementation</b></h5>
            <p>The implementation for the Mahalanobis classifier is very efficient. In addition to being pedagocially effective, the Cholesky Decomposition is known to be incredibly fast to compute. The Mahalanobis classifier is much faster than the K-th Nearest Neighbor classifier, taking a meager 18$\mu$s to determine the speaker versus the 34ms that the KNN algorithm takes. The determineSpeaker function simply takes in the covariance matrices and mean vectors for each speaker's database and an MFCC vector from the input signal. Then it calculates the Mahalanobis distance of the input MFCC vector with respect to both databases using Cholesky Decomposition, and finally returns the identity of the speaker that has the lesser Mahalanobis distance.</p>

            </p>
            <br><br>
            <font size = "0.5"> $^1$ For those interested, this orthonormal basis is actually the set of all eigenvectors of the covariance matrix $\mathbf{S}$, each divided by its respective eigenvalue. Additionally, $\mathbf{L}$ has the same eigenvectors as $\mathbf{S}$, with each eigenvalue being the square root of the respective eigenvalue of $\mathbf{S}$. 
            </font>

    </div>

    <!-- Results  -->
    <div class="container">
        <a name="results"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Experimental Results </h2>

        <a name="errorcorrection"></a>
        <br>
        <h3> Error Correction </h3>

    </div>

    <!-- Conclusions  -->
    <div class="container">
        <a name="conclusions"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> Conclusions </h2>

    </div>

    <!-- References  -->
    <div class="container">
        <a name="references"></a>
        <div class="clearfix"></div>
        <br><br>
        <hr class="featurette-divider">
        <h2> References </h2>
        <p> [1] M. Yankayis, "Feature Extraction Mel Frequency Cepstral Coefficients (MFFCs)," in YTU. Computer Engineering Department Website, 16-Nov-2016. </p><br>
        <p>[2] P. C. Mahalanobis, "On the generalized distance in statistics," Proc. Natl. Inst. Sci. India, vol. 2, no. 1, pp. 49-55, 1936. </p><br>
        <p>[3] K. Wojcicki, HTK MFCC MATLAB (Version 1.2). Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/</p><br>
        <p>[4] K. Wojcicki, Triangular Filterbank (Version 1.1). Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/ </p><br>
        <p>[5] K. Wojcicki, Framing Routines. Source code. Mathworks. 2011. https://www.mathworks.com/matlabcentral/fileexchange/ </p>
    </div>

    <!-- ***************** Footer ********************** -->
    <div class="clearfix"></div>
    <br><br>
    <hr class="featurette-divider">
    <footer>
        <p class="pull-right"><a href="#top">Back to top</a></p>
        <p>2016. Website built using Twitter Bootstrap.</p>
    </footer>
</div>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
<script src="website/js/bootstrap.min.js"></script>

</body>

</html>
